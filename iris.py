import os, sys, json, textwrap
from dataclasses import dataclass
from typing import List

from openai import OpenAI
import google.generativeai as genai
from dotenv import load_dotenv
from cache_manager import CacheManager

# Load API keys
load_dotenv()
MONICA_KEY = os.getenv("MONICA_API_KEY")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")

if not MONICA_KEY or not GEMINI_KEY:
    # Thay vÃ¬ sys.exit(1) Ä‘á»ƒ process khÃ´ng cháº¿t trÃªn platform nhÆ° Railway.
    print("âš ï¸ Missing API keys: MONICA_API_KEY or GEMINI_API_KEY not set. Set environment variables to enable AI calls.")
    MONICA_KEY = MONICA_KEY or ""
    GEMINI_KEY = GEMINI_KEY or ""

# Monica client (OpenAI compatible)
monica = OpenAI(base_url="https://openapi.monica.im/v1", api_key=MONICA_KEY)

# Gemini client
genai.configure(api_key=GEMINI_KEY)
gemini = genai.GenerativeModel("gemini-2.0-flash")

SYSTEM_GUIDELINES = """
Báº¡n lÃ  I.R.I.S â€” Intelligent Reliable Interactive Study Assistant.
NguyÃªn táº¯c:
- Tráº£ lá»i sÃºc tÃ­ch trÆ°á»›c, má»Ÿ rá»™ng khi cáº§n.
- CÃ´ng thá»©c/kÃ½ hiá»‡u nÃªn Ä‘áº·t trong \`\`\`...\`\`\`.
- Khi sinh quiz/flashcards: kÃ¨m Ä‘Ã¡p Ã¡n, giáº£i thÃ­ch ngáº¯n gá»n.
- Khi khÃ´ng cháº¯c: nÃªu giáº£ Ä‘á»‹nh/hÆ°á»›ng tá»± kiá»ƒm tra.
"""

cache = CacheManager()

# ====== Model Fallback Chain ======
def call_ai(prompt: str, temperature=0.6, function_type="general") -> str:
    # Generate cache key
    cache_key = cache.generate_cache_key(function_type, prompt=prompt, temperature=temperature)
    
    # Try to get from cache
    cached_result = cache.get_cache(cache_key)
    if cached_result:
        print(f"ðŸŽ¯ Cache HIT for {function_type}")
        return cached_result
    
    print(f"ðŸ”„ Cache MISS for {function_type} - calling AI...")
    
    models = ["gpt-4o-mini", "deepseek-chat"]
    for m in models:
        try:
            resp = monica.chat.completions.create(
                model=m,
                messages=[
                    {"role": "system", "content": SYSTEM_GUIDELINES},
                    {"role": "user", "content": prompt},
                ],
                temperature=temperature,
            )
            result = resp.choices[0].message.content.strip()
            cache.set_cache(cache_key, function_type, {"prompt": prompt, "temperature": temperature}, result)
            return result
        except Exception as e:
            print(f"âš ï¸ {m} failed: {e}")
            continue

    # Fallback cuá»‘i cÃ¹ng â†’ Gemini
    try:
        resp = gemini.generate_content([SYSTEM_GUIDELINES, prompt])
        result = resp.text.strip()
        cache.set_cache(cache_key, function_type, {"prompt": prompt, "temperature": temperature}, result)
        return result
    except Exception as e:
        print(f"âš ï¸ Gemini failed: {e}")
        return "âŒ All models failed. Please check your API keys or network connection."

def call_ai_stream(prompt: str, temperature=0.6, function_type: str = "general"):
    """Yield small text chunks from provider if streaming is supported; otherwise yield full text once."""
    # Try Monica (OpenAI compatible) with stream
    try:
        stream = monica.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_GUIDELINES},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            stream=True,
        )
        full_text = []
        for chunk in stream:
            try:
                delta = chunk.choices[0].delta.get("content") or ""
            except Exception:
                delta = ""
            if delta:
                full_text.append(delta)
                yield delta
        # Save to cache when done
        final = "".join(full_text).strip()
        if final:
            cache_key = cache.generate_cache_key(function_type, prompt=prompt, temperature=temperature)
            cache.set_cache(cache_key, function_type, {"prompt": prompt, "temperature": temperature}, final)
        return
    except Exception as e:
        print(f"âš ï¸ stream fallback: {e}")
    # Fallback: non-stream call, yield once
    text = call_ai(prompt, temperature=temperature, function_type=function_type)
    yield text

# ====== Data Structures ======
@dataclass
class Flashcard:
    front: str
    back: str


@dataclass
class QuizItem:
    question: str
    options: List[str]
    answer: str
    explanation: str


# ====== IRIS Class ======
class IRIS:
    def __init__(self):
        pass

    def chat(self, message: str) -> str:
        """General-purpose conversational reply, free-form.
        Keeps the educational tone but allows any topic from motivation to guidance.
        """
        prompt = message.strip()
        return call_ai(prompt, function_type="chat")

    def chat_stream(self, message: str):
        prompt = message.strip()
        for chunk in call_ai_stream(prompt, function_type="chat"):
            yield chunk

    def explain(self, question: str) -> str:
        prompt = f"Giáº£i thÃ­ch khÃ¡i niá»‡m hoáº·c cÃ¢u há»i há»c táº­p sau má»™t cÃ¡ch chi tiáº¿t nhÆ°ng sÃºc tÃ­ch:\n{question}"
        return call_ai(prompt, function_type="explain")

    def summarize(self, text: str, ratio: float = 0.4) -> str:
        prompt = f"TÃ³m táº¯t vÄƒn báº£n sau cÃ²n khoáº£ng {int(ratio*100)}% Ä‘á»™ dÃ i gá»‘c, giá»¯ nguyÃªn Ã½ chÃ­nh:\n{text}"
        return call_ai(prompt, function_type="summarize")

    def explain_code(self, code: str, language: str = "python") -> str:
        prompt = f"Giáº£i thÃ­ch mÃ£ code sau má»™t cÃ¡ch chi tiáº¿t, bao gá»“m chá»©c nÄƒng tá»«ng pháº§n vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng. NgÃ´n ngá»¯: {language}\n\`\`\`{'python' if language.lower() == 'python' else language}\n{code}\n\`\`\`"
        return call_ai(prompt, function_type="explain_code")

    def make_flashcards(self, text: str, n: int = 5) -> List[Flashcard]:
        prompt = f"""
        Táº¡o {n} flashcard á»Ÿ Ä‘á»‹nh dáº¡ng JSON THUáº¦N (chá»‰ JSON, khÃ´ng giáº£i thÃ­ch, khÃ´ng dÃ¹ng ```),
        má»—i pháº§n tá»­ cÃ³ cÃ¡c khÃ³a: "front" vÃ  "back" (vÃ  tÃ¹y chá»n "explanation").
        Náº¿u model sinh ra "question/answer" thÃ¬ hÃ£y Ä‘á»•i tÃªn thÃ nh "front/back".
        Ná»™i dung: {text}
        VÃ­ dá»¥: [{{"front":"Há»i?","back":"ÄÃ¡p","explanation":"â€¦"}}]
        """
        raw = call_ai(prompt, function_type="flashcards")
        try:
            data = json.loads(self._extract_json(raw))
            if isinstance(data, dict) and 'flashcards' in data:
                data = data['flashcards']
            normalized = []
            for item in data:
                if isinstance(item, dict):
                    front = item.get('front') or item.get('question') or item.get('q') or 'N/A'
                    back = item.get('back') or item.get('answer') or item.get('a') or 'N/A'
                    explanation = item.get('explanation') or item.get('note') or ''
                    normalized.append(Flashcard(front=front, back=back if explanation == '' else f"{back}\n\n{explanation}"))
            if not normalized:
                raise ValueError('Empty flashcards after normalization')
            return normalized[:n]
        except Exception:
            return [Flashcard(front="N/A", back=raw)]

    def quiz(self, text: str, n: int = 3) -> List[QuizItem]:
        prompt = f"""
        Táº¡o {n} cÃ¢u há»i tráº¯c nghiá»‡m, tráº£ vá» JSON THUáº¦N (chá»‰ JSON, khÃ´ng dÃ¹ng ```),
        má»—i pháº§n tá»­ cÃ³: "question" (string), "options" (array 3-5 lá»±a chá»n), "answer" (string Ä‘Ãºng), "explanation" (string ngáº¯n).
        Ná»™i dung: {text}
        VÃ­ dá»¥: [{{"question":"â€¦","options":["A","B"],"answer":"A","explanation":"â€¦"}}]
        """
        raw = call_ai(prompt, function_type="quiz")
        try:
            data = json.loads(self._extract_json(raw))
            if isinstance(data, dict) and 'quiz' in data:
                data = data['quiz']
            normalized: List[QuizItem] = []
            for item in data:
                if isinstance(item, dict):
                    question = item.get('question') or item.get('q') or 'N/A'
                    options = item.get('options') or item.get('choices') or []
                    answer = item.get('answer') or item.get('correct') or 'N/A'
                    explanation = item.get('explanation') or item.get('why') or ''
                    if not isinstance(options, list):
                        options = []
                    normalized.append(QuizItem(question=question, options=options, answer=answer, explanation=explanation))
            if not normalized:
                raise ValueError('Empty quiz after normalization')
            return normalized[:n]
        except Exception:
            return [QuizItem(question="N/A", options=[], answer="N/A", explanation=raw)]

    def study_plan(self, goal: str, days: int = 7, hours: int = 2) -> str:
        prompt = f"""
        Má»¥c tiÃªu: {goal}
        Láº­p káº¿ hoáº¡ch há»c trong {days} ngÃ y, má»—i ngÃ y {hours} giá».
        """
        return call_ai(prompt, function_type="study_plan")

    def _extract_json(self, text: str) -> str:
        """Extract JSON payload from LLM text.
        Supports blocks like ```json ... ```, ``json ... ``, or inline JSON mixed with prose.
        """
        import re

        if not text:
            return text

        s = text.strip()

        # 1) Try to find content inside code fences with 2-3 backticks
        fence_pattern = re.compile(r"`{2,3}[a-zA-Z]*\s*([\s\S]*?)`{2,3}")
        for match in fence_pattern.finditer(s):
            blk = match.group(1).strip()
            if blk.startswith('{') or blk.startswith('['):
                return blk

        # 2) Fallback: find first JSON-like substring { ... } or [ ... ]
        #    Try progressively larger spans until json.loads succeeds upstream
        bracket_pattern = re.compile(r"(\{[\s\S]*\}|\[[\s\S]*\])")
        candidates = bracket_pattern.findall(s)
        for cand in candidates:
            cand_strip = cand.strip()
            if cand_strip.startswith('{') or cand_strip.startswith('['):
                return cand_strip

        # 3) As last resort, return original trimmed text
        return s